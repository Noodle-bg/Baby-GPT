{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAOQHdNqJG5Blfvlvx3mxe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noodle-bg/F.R.I.E.N.D.S-GPT/blob/main/Baby_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0YCx1lqJg193"
      },
      "outputs": [],
      "source": [
        "with open('Friends_script.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NgIRCBnjSn2",
        "outputId": "eeb637c0-2815-4ad9-ff19-f314ee8f7dbe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4176958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRuOjkIwjVw_",
        "outputId": "9da4d213-9fe6-4587-c48a-ed759d7cfcab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz{|}¦­Éâäçèéíī–—‘’“”…€™�\n",
            "114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l:''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "print(encode(\"Hello world\"))\n",
        "print(decode(encode(\"Hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAPrdGHIjw-J",
        "outputId": "8c7bc33f-f57d-4e9a-c881-9aabe3f32be2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42, 69, 76, 76, 79, 2, 87, 79, 82, 76, 68]\n",
            "Hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data.shape,data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nQd3bvDkt8_",
        "outputId": "4145435d-7785-4f99-8e3d-0d27353751ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4176958]) torch.int64\n",
            "tensor([47, 49, 48, 43, 37, 35, 28,  2,  2, 54, 72, 69, 82, 69,  9, 83,  2, 78,\n",
            "        79, 84, 72, 73, 78, 71,  2, 84, 79,  2, 84, 69, 76, 76,  3,  2, 42, 69,\n",
            "         9, 83,  2, 74, 85, 83, 84,  2, 83, 79, 77, 69,  2, 71, 85, 89,  2, 43,\n",
            "         2, 87, 79, 82, 75,  2, 87, 73, 84, 72,  3,  1, 44, 49, 39, 59, 28,  2,\n",
            "         2, 37,  9, 77, 79, 78, 14,  2, 89, 79, 85,  9, 82, 69,  2, 71, 79, 73,\n",
            "        78, 71,  2, 79, 85, 84,  2, 87, 73, 84, 72,  2, 84, 72, 69,  2, 71, 85,\n",
            "        89,  3,  2, 54, 72, 69, 82, 69,  9, 83,  2, 71, 79, 84, 84, 65,  2, 66,\n",
            "        69,  2, 83, 79, 77, 69, 84, 72, 73, 78, 71,  2, 87, 82, 79, 78, 71,  2,\n",
            "        87, 73, 84, 72,  2, 72, 73, 77,  3,  1, 37, 42, 35, 48, 38, 46, 39, 52,\n",
            "        28,  2,  2, 53, 79,  2, 68, 79, 69, 83,  2, 72, 69,  2, 72, 65, 86, 69,\n",
            "         2, 65,  2, 72, 85, 77, 80, 33,  2, 35,  2, 72, 85, 77, 80,  2, 65, 78,\n",
            "        68,  2, 65,  2, 72, 65, 73, 82, 80, 73, 69, 67, 69, 33,  1, 50, 42, 49,\n",
            "        39, 36, 39, 28,  2,  2, 57, 65, 73, 84, 14,  2, 68, 79, 69, 83,  2, 72,\n",
            "        69,  2, 69, 65, 84,  2, 67, 72, 65, 76, 75, 33,  1, 50, 42, 49, 39, 36,\n",
            "        39, 28,  2,  2, 44, 85, 83, 84, 14,  2,  9, 67, 65, 85, 83, 69, 14,  2,\n",
            "        43,  2, 68, 79, 78,  9, 84,  2, 87, 65, 78, 84,  2, 72, 69, 82,  2, 84,\n",
            "        79,  2, 71, 79,  2, 84, 72, 82, 79, 85, 71, 72,  2, 87, 72, 65, 84,  2,\n",
            "        43,  2, 87, 69, 78, 84,  2, 84, 72, 82, 79, 85, 71, 72,  2, 87, 73, 84,\n",
            "        72,  2, 37, 65, 82, 76, 15,  2, 79, 72,  3,  1, 47, 49, 48, 43, 37, 35,\n",
            "        28,  2,  2, 49, 75, 65, 89, 14,  2, 69, 86, 69, 82, 89, 66, 79, 68, 89,\n",
            "         2, 82, 69, 76, 65, 88, 16,  2, 54, 72, 73, 83,  2, 73, 83,  2, 78, 79,\n",
            "        84,  2, 69, 86, 69, 78,  2, 65,  2, 68, 65, 84, 69, 16,  2, 43, 84,  9,\n",
            "        83,  2, 74, 85, 83, 84,  2, 84, 87, 79,  2, 80, 69, 79, 80, 76, 69,  2,\n",
            "        71, 79, 73, 78, 71,  2, 79, 85, 84,  2, 84, 79,  2, 68, 73, 78, 78, 69,\n",
            "        82,  2, 65, 78, 68, 15,  2, 78, 79, 84,  2, 72, 65, 86, 73, 78, 71,  2,\n",
            "        83, 12, 88, 16,  1, 37, 42, 35, 48, 38, 46, 39, 52, 28,  2,  2, 53, 79,\n",
            "        85, 78, 68, 83,  2, 76, 73, 75, 69,  2, 65,  2, 68, 65, 84, 69,  2, 84,\n",
            "        79,  2, 77, 69, 16,  1, 37, 42, 35, 48, 38, 46, 39, 52, 28,  2,  2, 35,\n",
            "        76, 82, 73, 71, 72, 84, 14,  2, 83, 79,  2, 43,  9, 77,  2, 66, 65, 67,\n",
            "        75,  2, 73, 78,  2, 72, 73, 71, 72,  2, 83, 67, 72, 79, 79, 76, 14,  2,\n",
            "        43,  9, 77,  2, 83, 84, 65, 78, 68, 73, 78, 71,  2, 73, 78,  2, 84, 72,\n",
            "        69,  2, 77, 73, 68, 68, 76, 69,  2, 79, 70,  2, 84, 72, 69,  2, 67, 65,\n",
            "        70, 69, 84, 69, 82, 73, 65, 14,  2, 65, 78, 68,  2, 43,  2, 82, 69, 65,\n",
            "        76, 73, 83, 69,  2, 43,  2, 65, 77,  2, 84, 79, 84, 65, 76, 76, 89,  2,\n",
            "        78, 65, 75, 69, 68, 16,  1, 35, 46, 46, 28,  2,  2, 49, 72, 14,  2, 89,\n",
            "        69, 65, 72, 16,  2, 42, 65, 68,  2, 84, 72, 65, 84,  2, 68, 82, 69, 65,\n",
            "        77, 16,  1, 37, 42, 35, 48, 38, 46, 39, 52, 28,  2,  2, 54, 72, 69, 78,\n",
            "         2, 43,  2, 76, 79, 79, 75,  2, 68, 79, 87, 78, 14,  2, 65, 78, 68,  2,\n",
            "        43,  2, 82, 69, 65, 76, 73, 83, 69,  2, 84, 72, 69, 82, 69,  9, 83,  2,\n",
            "        65,  2, 80, 72, 79, 78, 69, 16, 16, 16,  2, 84, 72, 69, 82, 69, 16,  1,\n",
            "        44, 49, 39, 59, 28,  2,  2, 43, 78, 83, 84, 69, 65, 68,  2, 79, 70, 16,\n",
            "        16, 16, 33,  1, 37, 42, 35, 48, 38, 46, 39, 52, 28,  2,  2, 54, 72, 65,\n",
            "        84,  9, 83,  2, 82, 73, 71, 72, 84, 16,  1, 44, 49, 39, 59, 28,  2,  2,\n",
            "        48, 69, 86, 69, 82,  2, 72, 65, 68,  2, 84, 72, 65, 84,  2, 68, 82, 69,\n",
            "        65, 77, 16,  1, 50, 42, 49, 39, 36, 39, 28,  2,  2, 48, 79, 16,  1, 37,\n",
            "        42, 35, 48, 38, 46, 39, 52, 28,  2,  2, 35, 76, 76,  2, 79, 70,  2, 65,\n",
            "         2, 83, 85, 68, 68, 69, 78, 14,  2, 84, 72, 69,  2, 80, 72, 79, 78, 69,\n",
            "         2, 83, 84, 65, 82, 84, 83,  2, 84, 79,  2, 82, 73, 78, 71, 16,  2, 35,\n",
            "        78, 68,  2, 73, 84,  2, 84, 85, 82, 78, 83,  2, 79, 85, 84,  2, 73, 84,\n",
            "         9, 83,  2, 77, 89,  2, 77, 79, 84, 72, 69, 82, 14,  2, 87, 72, 73, 67,\n",
            "        72,  2, 73, 83,  2, 86, 69, 82, 89,  2, 87, 69, 73, 82, 68, 14,  2, 66,\n",
            "        69, 67, 65, 85, 83, 69, 15,  2, 83, 72, 69,  2, 78, 69, 86, 69, 82,  2,\n",
            "        67, 65, 76, 76, 83,  2, 77, 69,  3,  1, 52, 49, 53, 53, 28,  2,  2, 10,\n",
            "        47, 49, 52, 54, 43, 40, 43, 39, 38, 11,  2, 42, 73, 16,  1, 44, 49, 39,\n",
            "        59, 28,  2,  2, 54, 72, 73, 83,  2, 71, 85, 89,  2, 83, 65, 89, 83,  2,\n",
            "        72, 69, 76, 76, 79, 14,  2, 43,  2, 87])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data=data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "MqGxkS15lEDt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "batch_size = 4 # How much parallel computation happens at a time\n",
        "block_size = 8 # Context length\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size]for i in ix])\n",
        "  y= torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "  return x,y\n"
      ],
      "metadata": {
        "id": "B3F08KEdlhRt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb,yb = get_batch('train')\n",
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ww3Ub37pZI5",
        "outputId": "0b1f69f1-4ef6-4d1c-934e-9886fd560b74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[89,  2, 65, 82, 69,  2, 71, 79],\n",
            "        [ 2,  2, 47, 69, 33,  2, 43,  9],\n",
            "        [83,  2, 71, 79,  2, 84, 79,  2],\n",
            "        [54, 79,  2, 52, 65, 67, 72, 69]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits,loss\n",
        "\n",
        "\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits,loss = self(idx)\n",
        "      # Focus only on the last time step\n",
        "      logits = logits[:,-1,:] # Becomes (B,C)\n",
        "      probs = F.softmax(logits,dim = -1) # (B,C)\n",
        "      idx_next = torch.multinomial(probs,num_samples = 1) # (B,1)\n",
        "      idx = torch.cat((idx,idx_next),dim = 1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits , loss = m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,1),dtype = torch.long),max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reUZmqlfppiw",
        "outputId": "b4c31270-1497-4a0b-d103-94cfaadc47df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 114])\n",
            "tensor(5.0856, grad_fn=<NllLossBackward0>)\n",
            "\t.>­¦IYyY­|¦īw2”VK5P;JNBfNkA`OIWn|â)=1‘\téè1[wíä`rxoéèT?­Tj5Lä<\t­!:’yI¦R’y W?i€€[èī™JMg”H�q5!1D.ävéçtl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "hcTHYBCtrLFj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "  xb,yb = get_batch('train')\n",
        "  logits , loss= m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POR7hpagvuF6",
        "outputId": "b98477f1-92a8-459c-a8dc-0f381cfdcf16"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3311877250671387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1),dtype = torch.long),max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCMurj-SwNL0",
        "outputId": "7cad67a9-a1bb-4ce0-8d06-5966e1354541"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tbonosmmouretit  hutha le met_DLER: pemed, simitesod'mere*43–qul—oweaschin8çxtot thenoup.\n",
            "ROhay? myoo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Self attention model\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "B,T,C =4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "\n",
        "\n",
        "# so for a single head\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size,bias = False)\n",
        "query = nn.Linear(C,head_size,bias = False)\n",
        "value = nn.Linear(C,head_size, bias  = False)\n",
        "\n",
        "k  = key(x) # (B,T,head_size)\n",
        "q = query(x) # (B,T,head_size)\n",
        "\n",
        "wei = q @ k.transpose(-1,-2) # (B, T,16) x (B,16,T) -> (B,T,T)\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei,dim = -1)\n",
        "v = value(x)\n",
        "out = wei @ v # (B,T,16)\n",
        "out.shape"
      ],
      "metadata": {
        "id": "-3Qb8V7uwn2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3803a5ea-59f7-454f-e193-e002e3647f1f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# Hyper Parameters\n",
        "\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(42)\n",
        "print(device)\n",
        "\n",
        "with open('Friends_script.txt','r',encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l:''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train_data=data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size]for i in ix])\n",
        "    y= torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "    return x,y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train','val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X,Y = get_batch(split)\n",
        "            X,Y = X.to(device), Y.to(device)\n",
        "            logits,loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" Implements one head of self attention \"\"\"\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.value = nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.query = nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "        B,T,C  = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "\n",
        "        wei = q @ k.transpose(-1,-2) * C**(-0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "        wei = F.softmax(wei,dim=  -1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of self attention in parallel\"\"\"\n",
        "    def __init__(self,num_heads,head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads*head_size,n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=  torch.cat([h(x) for h in self.heads],dim= -1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Feed forward layer . Note -> Feed forward network works at a token level (independent)\"\"\"\n",
        "\n",
        "    def __init__(self,n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd,n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block (decoder sec) : Communication followed by computation\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self,n_embd,n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head,head_size=head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x = self.sa(x)\n",
        "        # x = self.ffwd(x)\n",
        "        x = self.ln1(x)\n",
        "        x = x+ self.sa(x)\n",
        "        x = self.ln2(x)\n",
        "        x = x + self.ffwd(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "        # self.sa_heads = MultiHeadAttention(4,n_embd//4)  # Head -> 32 dim, now split into 4 sets of 8 dim\n",
        "        # self.ffwd = FeedForward(n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self,idx,targets=None):\n",
        "\n",
        "\n",
        "        B,T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T,device=device)) # (T,C)\n",
        "        x = token_emb+pos_emb  # (B,T,C)\n",
        "        # x = self.sa_heads(x)\n",
        "        # x = self.ffwd(x)\n",
        "        x = self.blocks(x)\n",
        "        logits =self.lm_head(x)  # (B,T,vocab_size)\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T,C = logits.shape\n",
        "            logits = logits.view(B*T,C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits,targets)\n",
        "        return logits,loss\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self,idx,max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to only the last block_size tokens\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "            logits,loss = self(idx_cond)\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:,-1,:] # Becomes (B,C)\n",
        "            probs = F.softmax(logits,dim = -1) # (B,C)\n",
        "            idx_next = torch.multinomial(probs,num_samples = 1) # (B,1)\n",
        "            idx = torch.cat((idx,idx_next),dim = 1) #(B,T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Epoch : {iter} | train loss : {losses['train']:.4f} | val losses: {losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "    xb,yb = get_batch('train')\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    logits,loss = model(xb,yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1,1),dtype = torch.long,device = device)\n",
        "print(decode(m.generate(context,max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v169Ael-9C8G",
        "outputId": "32234032-2f9a-4713-f908-ed154dafe33d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Epoch : 0 | train loss : 4.8632 | val losses: 4.8694\n",
            "Epoch : 500 | train loss : 1.7259 | val losses: 1.7727\n",
            "Epoch : 1000 | train loss : 1.4086 | val losses: 1.4732\n",
            "Epoch : 1500 | train loss : 1.2860 | val losses: 1.3637\n",
            "Epoch : 2000 | train loss : 1.2121 | val losses: 1.2948\n",
            "Epoch : 2500 | train loss : 1.1668 | val losses: 1.2548\n",
            "Epoch : 3000 | train loss : 1.1256 | val losses: 1.2204\n",
            "Epoch : 3500 | train loss : 1.0986 | val losses: 1.2021\n",
            "Epoch : 4000 | train loss : 1.0718 | val losses: 1.1732\n",
            "Epoch : 4500 | train loss : 1.0509 | val losses: 1.1609\n",
            "\tal, always the water matake? As a week resquare it of matchia's fine.\n",
            "ROSS:  I am so sorry...\n",
            "PHOEBE:  All right, facaus...Ince we pick up when the entire with pictures out of my other cout enterpective, and senny, anyway, what about what he you did, like?\n",
            "ROSS:  Long?\n",
            "PHOEBE:  (to Rachel) Too. That’s now?\n",
            "RACHEL:  That could’ve been worsed\n",
            "PHOEBE:  You whould…I save it looks like final on you?!\n",
            "RACHEL:  You lose him?\n",
            "MONICA:  Ohh, I mean your bad. I’m so good, I’m gonna be milking about funny. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1),dtype = torch.long,device = device)\n",
        "print(decode(m.generate(context,max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn4ZgzaSdhbW",
        "outputId": "e761ddf6-d4bb-4d32-e244-130a7c13873e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ting this thing I’m gonna call along, and if…there are scarastas in the bedreet don’t ex!\n",
            "RACHEL:  Come tonight!\n",
            "MANNY:  You said your cheese a largelet holding.\n",
            "RACHEL:  Don’t just an isslack such Tpen them! But, now he misssed outside and-all then sopable of they to hold never next goody.\n",
            "CHANDLER:  Wonder you instrading; those weeks in the dational light now.\n",
            "MAN:  I get out to here, a new guy, dancer what-was.\n",
            "ROSS:  I know know, I knew you and on hole that empty-pancy, because we said that last, at there again. Oh, look, um, I remember home, you know, it is napple either, and that's sore partying you doing the chick on your just tired, you et, and you didn't take that I'm not withing me, this guy, he loved and.... and no, tell you that white doing shot commes about I can't cry. You love to nap stips in cascar, okay?.. and we will come tocking on thus to I cantince, and Repost for or an elicial scrows...a fire sconwstick.\n",
            "JANICE:  (clossing Carol) Stay Foldie Srogol.\n",
            "JANICE:  Rexcus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.ones((1,1),dtype = torch.long,device = device)\n",
        "print(decode(m.generate(context,max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gakDbOqq7lk",
        "outputId": "e7f7dde6-8d2f-4137-895c-1a4e184b22f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "JOEY:  You know, but then!! [Joey and Mona still looking at it!\n",
            "PHOEBE:  I'll be a better! (Chandler does follows behind and he disgusting thing (points his room) I will, will you die and fell welling you with you, but I'll go over to give 25 out of you to get a face! (on the pause) Aw?!\n",
            "ROSS:  I got this baby!\n",
            "CHANDLER:  Y'know that been up, that works! I just left this cat!\n",
            "ROSS:  Excuse me!\n",
            "CHANDLER:  [to the going at head] Isn't that all manI for you?\n",
            "ROB:  You are by Richards's storane who you're scarying lists…\n",
            "DR. BREED BY:  By each! Don’t talk tomats!\n",
            "CHANDLER:  Oh god! No well then, he'd tough that all my neighbon’s .\n",
            "JOEY:  He's party!\n",
            "RICHARD:  How hard pal.\n",
            "DR. BREDDIE:  Sure, voice we're testae things all unounhtil lets you guys, and an one fancy pants\n",
            "JOEY:  (looking at this) She's working at a salsocking direction! I think this boss yards off.\n",
            "DR. BOWN:  (to check) Joey, I was talking about!!!\n",
            "CHANDLER:  There are gonna be here?!\n",
            "RACHEL:  The recerinisms she room. You sa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and optimizer state\n",
        "model_save_path = \"friends_language_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk5BfG69rV0l",
        "outputId": "088ec604-9fe7-415f-e6c9-199965d5f938"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to friends_language_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpvxb67ar50s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}